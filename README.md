# Measuring faithfulness and performance in mixture-of-experts language models through expert misrouting

## Overview
This repository contains the initial scaffold for my Individual Project (2025/26).  
The project will explore **how mixture-of-experts (MoE) language models behave when their routing decisions are intentionally altered**. The aim is to study whether models remain **faithful to their internal computation** when producing explanations, and how performance changes under controlled misrouting.

At this stage, the repository only includes a basic folder structure.  
No code, experiments, or models have been added yet.

## Goals (High-Level)
- Investigate how MoE router decisions influence model predictions.  
- Identify experts that contribute to specific reasoning behaviours.  
- Apply controlled **expert misrouting** to probe the modelâ€™s internal computation.  
- Measure **faithfulness**, defined as the alignment between internal causal pathways and external explanations.  
- Compare faithfulness with task performance across different intervention types.

These goals will be refined as the project develops.

## Repository Structure
