{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea5a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6317d0ee7da4c38a0a6713adf7e98be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42d9980473447b8a3bce39984919f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615c159732d34b9fb0ef4ea7e28f0620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b33f644474d46cab2f27f1d9b3fe981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/2.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f389c51de747a6a26b90d8d19e7811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da8dbf56a8e4c08b5cbe01e86685b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22398a136a8743fea4c99de371e6b95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '570715c09e06ca38007e93d8',\n",
       " 'title': 'Chihuahua_(state)',\n",
       " 'context': 'On February 8, 1847, Doniphan continued his march with 924 men mostly from Missouri; he accompanied a train of 315 wagons of a large commercial caravan heading to the state capital. Meanwhile, the Mexican forces in the state had time to prepare a defense against the Americans. About 20 miles (32 km) north of the capital where two mountain ranges join from east to west is the only pass into the capital; known as Sacramento Pass, this point is now part of present-day Chihuahua City. The Battle of Sacramento was the most important battle fought in the state of Chihuahua because it was the sole defense for the state capital. The battle ended quickly because of some devastating defensive errors from the Mexican forces and the ingenious strategic moves by the American forces. After their loss at the Battle of Sacramento, the remaining Mexican soldiers retreated south, leaving the city to American occupation. Almost 300 Mexicans were killed in the battle, as well as almost 300 wounded. The Americans also confiscated large amounts of Mexican supplies and took 400 Mexican soldiers prisoners of war. American forces maintained an occupation of the state capital for the rest of the Mexican–American War.',\n",
       " 'question': 'Which was the most important battle fought in Chihuahua?',\n",
       " 'answers': {'answer_start': [486], 'text': ['The Battle of Sacramento']}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset \n",
    "\n",
    "squad = load_dataset(\"decodingchris/clean_squad_v2\", split = \"train\")\n",
    "# shuffle the dataset\n",
    "squad = squad.shuffle(seed=67)\n",
    "squad[9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Question: How many years do momotremes and therian mammals go back?\n",
      "Context: If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. Ambondro is more closely related to monotremes than to therian mammals while Amphilestes and Amphitherium are more closel ...\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 1\n",
      "Question: What language did Gorals speak?\n",
      "Context: The Gorals of southern Poland and northern Slovakia are partially descended from Romance-speaking Vlachs who migrated into the region from the 14th to 17th centuries and were absorbed into the local population. The population of Moravian Wallachia also descend of this population. ...\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 2\n",
      "Question: uring what period was a cappella losinging popularity as religious music?\n",
      "Context: A cappella music was originally used in religious music, especially church music as well as anasheed and zemirot. Gregorian chant is an example of a cappella singing, as is the majority of secular vocal music from the Renaissance. The madrigal, up until its development in the early Baroque into an i ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Select only the question and context columns\n",
    "pairs = squad.select_columns([\"question\", \"context\"])\n",
    "\n",
    "\"\"\"Preview a few examples from the dataset.\n",
    "This loop prints the first 3 (index 0, 1, 2) question-context pairs\n",
    "in a readable format without causing indexing errors.\n",
    "\"\"\"\n",
    "for idx, example in enumerate(pairs):\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(\"Question:\", example[\"question\"])\n",
    "    print(\"Context:\", example[\"context\"][:300], \"...\")  # truncate long contexts\n",
    "    print(\"-\" * 80)\n",
    "    if idx >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65d44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 130316\n",
      "Number of columns in the dataset: 2\n"
     ]
    }
   ],
   "source": [
    "#Print number of rows in the dataset\n",
    "print(f\"Number of rows in the dataset: {len(pairs)}\")\n",
    "print(f\"Number of columns in the dataset: {len(pairs.column_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dea0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must answer the question using ONLY the information provided in the context below.\n",
      "If the answer cannot be determined from the context, respond with \"Not answerable from the given context.\"\n",
      "Do not use any external knowledge.\n",
      "\n",
      "Context:\n",
      "The 1960s and 1970s saw an acceleration in the decolonisation of Africa and the Caribbean. Over 20 countries gained independence from Britain as part of a planned transition to self-government. In 1965, however, the Rhodesian Prime Minister, Ian Smith, in opposition to moves toward majority rule, declared unilateral independence from Britain while still expressing \"loyalty and devotion\" to Elizabeth. Although the Queen dismissed him in a formal declaration, and the international community applied sanctions against Rhodesia, his regime survived for over a decade. As Britain's ties to its former empire weakened, the British government sought entry to the European Community, a goal it achieved in 1973.\n",
      "\n",
      "Question:\n",
      "How many countries got independence from Britain during decolonization?\n"
     ]
    }
   ],
   "source": [
    "# This cell appends a cue to each question\n",
    "\n",
    "def build_x1_prompt(question, context):\n",
    "    instruction = (\n",
    "        \"You must answer the question using ONLY the information provided in the context below.\\n\"\n",
    "        \"If the answer cannot be determined from the context, respond with \"\n",
    "        \"\\\"Not answerable from the given context.\\\"\\n\"\n",
    "        \"Do not use any external knowledge.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        instruction\n",
    "        + \"Context:\\n\"\n",
    "        + context\n",
    "        + \"\\n\\nQuestion:\\n\"\n",
    "        + question\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "example = pairs[27]\n",
    "x1_prompt = build_x1_prompt(example[\"question\"], example[\"context\"])\n",
    "print(x1_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610901ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to access the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc8a8a",
   "metadata": {},
   "source": [
    "## Goal: Build a DeepSeek V2 Lite Input Dataset\n",
    "\n",
    "In the next steps, we want to construct our **own dataset** that can be used as input for **DeepSeek V2 Lite**. This dataset will consist of **pairwise entries** derived from the SQuAD-style question–context data we explored above.\n",
    "\n",
    "Concretely, we will create examples of three types:\n",
    "- A **cue for context inspection**: an instruction-style prompt that tells the model how to read and use the context.\n",
    "- A **context + question prompt**: the context and question combined into a single prompt (as in `build_x1_prompt(question, context)` above).\n",
    "- A **question-only variant**: the same question without its context, to study how removing the context affects model behaviour.\n",
    "\n",
    "Our immediate goal is to **actually build this dataset and prepare it for input into DeepSeek V2 Lite**. To keep things simple and inspectable, we will start by constructing a **small subset** of the full data (e.g., only rows 0–100). Once this subset looks correct and works end to end, we can later scale up to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca209af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset has been written to /content/drive/MyDrive/Individual-Project-25-26/stage1/data/Prompt_context.jsonl and /content/drive/MyDrive/Individual-Project-25-26/stage1/data/Prompt_base.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Write the processed dataset to a new JSONL file\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory in Google Drive (adjust folder name if you like)\n",
    "drive_base = Path(\"/content/drive/MyDrive/Individual-Project-25-26/stage1/data\")\n",
    "drive_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file1 = drive_base / \"Prompt_context.jsonl\"\n",
    "output_file2 = drive_base / \"Prompt_base.jsonl\"\n",
    "\n",
    "def build_x1_prompt(question, context):\n",
    "    instruction = (\n",
    "        \"You must answer the question using ONLY the information provided in the context below.\\n\"\n",
    "        \"If the answer cannot be determined from the context, respond with \"\n",
    "        \"\\\"Not answerable from the given context.\\\"\\n\"\n",
    "        \"Do not use any external knowledge.\\n\\n\"\n",
    "    )\n",
    "    prompt = (\n",
    "        instruction\n",
    "        + \"Context:\\n\"\n",
    "        + context\n",
    "        + \"\\n\\nQuestion:\\n\"\n",
    "        + question\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "with open(output_file1, 'w', encoding=\"utf-8\") as f1, open(output_file2, 'w', encoding=\"utf-8\") as f2:\n",
    "    for idx in range(0, 100):\n",
    "        example = pairs[idx]\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "\n",
    "        x1_prompt = build_x1_prompt(question, context)\n",
    "        x2_prompt = question\n",
    "\n",
    "        json.dump({\"prompt\": x1_prompt}, f1, ensure_ascii=False)\n",
    "        f1.write('\\n')\n",
    "\n",
    "        json.dump({\"prompt\": x2_prompt}, f2, ensure_ascii=False)\n",
    "        f2.write('\\n')\n",
    "\n",
    "if output_file1.exists() and output_file1.stat().st_size > 0 and \\\n",
    "   output_file2.exists() and output_file2.stat().st_size > 0:\n",
    "    print(f\"Processed dataset has been written to {output_file1} and {output_file2}\")\n",
    "else:\n",
    "    print(\"Error: One or both output files were not created or are empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676e12b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
