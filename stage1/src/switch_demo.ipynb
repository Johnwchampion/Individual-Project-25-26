{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d07dd09",
   "metadata": {},
   "source": [
    "# Preliminary MoE Routing Demo (Switch Transformer)\n",
    "\n",
    "This notebook is a small exploratory test to understand **Mixture-of-Experts (MoE) routing** in a real model.\n",
    "\n",
    "Using **Switch-Base-8**, we inspect **token-level expert routing decisions** at a **single encoder MoE layer**.  \n",
    "We capture router logits, selected experts, and routing entropy for short inputs, and align them with the tokenizer’s actual subword tokens.\n",
    "\n",
    "This notebook is **purely observational**:\n",
    "- no training\n",
    "- no fine-tuning\n",
    "- no architectural modification\n",
    "\n",
    "The goal is simply to verify that expert routing can be intercepted and interpreted before scaling to larger MoE models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddbe00b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f847f2de0b04be3b2d1d28a7823b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch MoE loaded correctly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/switch-base-8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Switch MoE loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd20e5d",
   "metadata": {},
   "source": [
    "List all modules with \"router\" in the name to find the routing components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8f8a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 10 19:00:06 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   50C    P0             28W /   70W |    4360MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d36df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.1.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.1.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.3.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.3.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.5.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.5.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.7.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.7.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.9.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.9.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.11.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.11.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.1.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.1.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.3.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.3.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.5.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.5.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.7.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.7.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.9.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.9.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.11.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.11.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "Switch MoE router modules identified.\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"router\" in name.lower():\n",
    "        print(name, \"->\", type(module))\n",
    "print(\"Switch MoE router modules identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58b237",
   "metadata": {},
   "source": [
    "Tokenize a single example and print ids, tokens, and attention mask to see the subword split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d9cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "question: Where is Paris?, context: Paris is the capital of France.\n",
      "\n",
      "Token IDs:\n",
      "[822, 10, 2840, 19, 1919, 58, 6, 2625, 10, 1919, 19, 8, 1784, 13, 1410, 5, 1]\n",
      "\n",
      "Tokens:\n",
      "['▁question', ':', '▁Where', '▁is', '▁Paris', '?', ',', '▁context', ':', '▁Paris', '▁is', '▁the', '▁capital', '▁of', '▁France', '.', '</s>']\n",
      "\n",
      "Attention Mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: AttributeError(\"'str' object has no attribute 'decode'\")\n"
     ]
    }
   ],
   "source": [
    "# Minimal step: tokenize a single input and inspect tokens\n",
    "text = \"question: Where is Paris?, context: Paris is the capital of France.\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"][0].tolist()\n",
    "attention_mask = encoding[\"attention_mask\"][0].tolist()\n",
    "\n",
    "print(\"Input text:\")\n",
    "print(text)\n",
    "print(\"\\nToken IDs:\")\n",
    "print(input_ids)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd43a6",
   "metadata": {},
   "source": [
    "Move tokenized tensors onto the same device as the model. The model might be on GPU while the tokenizer outputs are on CPU, and PyTorch requires both to be on the same device for a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ef483bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "attention_mask device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-auto_conversion:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 116, in auto_conversion\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 95, in auto_conversion\n",
      "    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 76, in get_conversion_pr_reference\n",
      "    raise OSError(\n",
      "OSError: Could not create safetensors conversion PR. The repo does not appear to have a file named pytorch_model.bin or model.safetensors.If you are loading with variant, use `use_safetensors=False` to load the original model.\n"
     ]
    }
   ],
   "source": [
    "# Move tokenized tensors to the model device\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"].to(model.device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(model.device)\n",
    "\n",
    "print(\"input_ids device:\", input_ids.device)\n",
    "print(\"attention_mask device:\", attention_mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b914dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output last_hidden_state: torch.Size([1, 17, 768])\n",
      "This consists of the batch dimension (1), sequence length, and hidden state dimension.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Encoder forward only (no hooks)\n",
    "with torch.no_grad(), torch.autocast(model.device.type, dtype=model.dtype):\n",
    "    encoder_outputs = model.encoder(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "print(\"Encoder output last_hidden_state:\", encoder_outputs.last_hidden_state.shape)\n",
    "print(\"This consists of the batch dimension (1), sequence length, and hidden state dimension.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e9194",
   "metadata": {},
   "source": [
    "\n",
    "### Forward hook method\n",
    "\n",
    "\n",
    "- **`router_classifier`**  \n",
    "  The **linear layer inside the router** that maps each token’s hidden state to logits over the experts  \n",
    "  (shape: `(hidden_dim, num_experts)`).\n",
    "\n",
    "- **`classifier_outputs`**  \n",
    "  A list used to **store the outputs captured by the hook**.  \n",
    "  Hooks may fire multiple times, so outputs are accumulated safely here.\n",
    "\n",
    "- **`classifier_hook(module, module_inputs, module_outputs)`**  \n",
    "  A function that PyTorch **automatically calls when the classifier runs**.\n",
    "  - `module`: the classifier layer itself (`nn.Linear`)\n",
    "  - `module_inputs`: hidden-state tensors **entering the classifier**  \n",
    "    (one vector per token)\n",
    "  - `module_outputs`: **raw logits over experts**  \n",
    "    (shape: `[num_tokens, num_experts] so (17,8) `)\n",
    "\n",
    "- **Tuple handling inside the hook**  \n",
    "  Some modules return `(output, extra_info)`.  \n",
    "  This unwraps the tensor so we always store just the logits.\n",
    "\n",
    "- **`register_forward_hook(...)`**  \n",
    "  Attaches the hook to the classifier.  \n",
    "  From this point on, **whenever the classifier executes**, the hook is triggered.\n",
    "\n",
    "- **`with torch.no_grad()`**  \n",
    "  Disables gradient tracking because we are **observing, not training**.  \n",
    "  Saves memory and computation.\n",
    "\n",
    "- **`torch.autocast(...)`**  \n",
    "  Runs the forward pass in the model’s native precision (e.g. FP16 on GPU).  \n",
    "  Prevents dtype mismatches and is standard practice for inference.\n",
    "\n",
    "- **`model.encoder(...)`**  \n",
    "  Executes a single encoder forward pass.  \n",
    "  This is what **actually triggers the router and the hook**.\n",
    "\n",
    "- **`handle.remove()`**  \n",
    "  Detaches the hook immediately after the pass to avoid duplicate captures.\n",
    "\n",
    "- **`classifier_outputs[0]`**  \n",
    "  The captured tensor of **per-token expert logits**.  \n",
    "  Each row corresponds to one token, each column to one expert.\n",
    "\n",
    "- **Final prints**  \n",
    "  Confirm tensor shape and inspect logits for a specific token and all experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6aecb2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier logits shape: (17, 8)\n",
      "Classifier logits sample: [0.05078125, -2.0, -0.17578125, 2.8125, -0.154296875, -0.173828125, 0.470703125, 0.279296875]\n"
     ]
    }
   ],
   "source": [
    "# Hook the router classifier to capture per-expert logits\n",
    "router_classifier = model.encoder.block[1].layer[1].mlp.router.classifier\n",
    "classifier_outputs = []\n",
    "\n",
    "# Define a hook function to capture the outputs of the router classifier\n",
    "def classifier_hook(module, module_inputs, module_outputs):\n",
    "    if isinstance(module_outputs, tuple):\n",
    "        #Ensure we only capture the logits \n",
    "        module_outputs = module_outputs[0]\n",
    "    classifier_outputs.append(module_outputs)\n",
    "\n",
    "\n",
    "handle = router_classifier.register_forward_hook(classifier_hook)\n",
    "\n",
    "with torch.no_grad(), torch.autocast(model.device.type, dtype=model.dtype):\n",
    "    _ = model.encoder(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "logits = classifier_outputs[0].cpu()\n",
    "print(\"Classifier logits shape:\", tuple(logits.shape))\n",
    "print(\"Classifier logits sample:\", logits[13, :8].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed03e3f",
   "metadata": {},
   "source": [
    "Convert logits into a concrete routing summary. We take the argmax to pick the top expert per token, compute entropy to measure confidence, then align those values with the tokenizer's subword tokens in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bf5c222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-203db62e-2cb7-4fe1-86fc-1486e65d52a5\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>EXPERT</th>\n",
       "      <th>LOGIT</th>\n",
       "      <th>ENTROPY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁question</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.733095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:</td>\n",
       "      <td>2</td>\n",
       "      <td>2.468750</td>\n",
       "      <td>1.472535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁Where</td>\n",
       "      <td>7</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>1.368644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁is</td>\n",
       "      <td>4</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>1.297853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁Paris</td>\n",
       "      <td>7</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>1.361608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2.359375</td>\n",
       "      <td>1.484937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>1.598489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>▁context</td>\n",
       "      <td>5</td>\n",
       "      <td>2.656250</td>\n",
       "      <td>1.720571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>:</td>\n",
       "      <td>2</td>\n",
       "      <td>2.796875</td>\n",
       "      <td>1.355670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>▁Paris</td>\n",
       "      <td>7</td>\n",
       "      <td>2.921875</td>\n",
       "      <td>1.281017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>▁is</td>\n",
       "      <td>4</td>\n",
       "      <td>2.453125</td>\n",
       "      <td>1.445303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>▁the</td>\n",
       "      <td>6</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>1.566154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>▁capital</td>\n",
       "      <td>4</td>\n",
       "      <td>2.468750</td>\n",
       "      <td>1.167180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>▁of</td>\n",
       "      <td>3</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.117690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>▁France</td>\n",
       "      <td>7</td>\n",
       "      <td>2.468750</td>\n",
       "      <td>1.105413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>2</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>1.586292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>1.226562</td>\n",
       "      <td>1.775595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-203db62e-2cb7-4fe1-86fc-1486e65d52a5')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-203db62e-2cb7-4fe1-86fc-1486e65d52a5 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-203db62e-2cb7-4fe1-86fc-1486e65d52a5');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        TOKEN  EXPERT     LOGIT   ENTROPY\n",
       "0   ▁question       4  2.000000  1.733095\n",
       "1           :       2  2.468750  1.472535\n",
       "2      ▁Where       7  2.375000  1.368644\n",
       "3         ▁is       4  2.718750  1.297853\n",
       "4      ▁Paris       7  2.265625  1.361608\n",
       "5           ?       2  2.359375  1.484937\n",
       "6           ,       2  2.062500  1.598489\n",
       "7    ▁context       5  2.656250  1.720571\n",
       "8           :       2  2.796875  1.355670\n",
       "9      ▁Paris       7  2.921875  1.281017\n",
       "10        ▁is       4  2.453125  1.445303\n",
       "11       ▁the       6  1.812500  1.566154\n",
       "12   ▁capital       4  2.468750  1.167180\n",
       "13        ▁of       3  2.812500  1.117690\n",
       "14    ▁France       7  2.468750  1.105413\n",
       "15          .       2  1.953125  1.586292\n",
       "16       </s>       7  1.226562  1.775595"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute expert assignment + entropy per token\n",
    "logits = logits.float()\n",
    "experts = logits.argmax(dim=-1)\n",
    "entropy = -(F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)).sum(dim=-1)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "df = pd.DataFrame({\n",
    "    \"TOKEN\": tokens,\n",
    "    \"EXPERT\": experts.cpu().numpy(),\n",
    "    \"LOGIT\": logits.max(dim=-1).values.cpu().numpy(),\n",
    "    \"ENTROPY\": entropy.cpu().numpy()\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48defb64",
   "metadata": {},
   "source": [
    "Next we prepare the contents of Prompts_base and Prompts_context for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18c7da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount to drive\n",
    "\n",
    "import google.colab.drive as drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8265ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(\"/content/drive/MyDrive/Individual-Project-25-26/stage1/data\")\n",
    "base_path = data_dir / \"Prompt_base.jsonl\"\n",
    "context_path = data_dir / \"Prompt_context.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f392372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base prompts: 100\n",
      "Loaded context prompts: 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_prompt_records(path):\n",
    "    records = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "def build_context_prompt(item):\n",
    "    cue = item.get(\"Cue\", \"\")\n",
    "    context = item.get(\"Context\", \"\")\n",
    "    question = item.get(\"Question\", \"\")\n",
    "    parts = []\n",
    "    if cue:\n",
    "        parts.append(cue)\n",
    "    parts.append(\"Context:\\n\" + context)\n",
    "    parts.append(\"Question:\\n\" + question)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "base_records = load_prompt_records(base_path)\n",
    "context_records = load_prompt_records(context_path)\n",
    "\n",
    "base_prompts = [record.get(\"Question\", \"\") for record in base_records]\n",
    "context_prompts = [build_context_prompt(record) for record in context_records]\n",
    "context_questions = [record.get(\"Question\", \"\") for record in context_records]\n",
    "\n",
    "prompt_sets = {\n",
    "    \"base\": base_prompts,\n",
    "    \"context\": context_prompts,\n",
    "}\n",
    "\n",
    "print(\"Loaded base prompts:\", len(base_prompts))\n",
    "print(\"Loaded context prompts:\", len(context_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c731df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base tokenized: 100\n",
      "Context tokenized: 100\n"
     ]
    }
   ],
   "source": [
    "def tokenize_prompts(prompts):\n",
    "    tokenized = []\n",
    "    for prompt in prompts:\n",
    "        encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        tokenized.append({\n",
    "            \"input_ids\": encoding[\"input_ids\"][0].tolist(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"][0].tolist(),\n",
    "        })\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "base_tokenized = tokenize_prompts(base_prompts)\n",
    "context_tokenized = tokenize_prompts(context_prompts)  # Fixed: was context_questions, now context_prompts\n",
    "\n",
    "print(\"Base tokenized:\", len(base_tokenized))\n",
    "print(\"Context tokenized:\", len(context_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "241075f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base example 99:\n",
      "prompt: Where was the city originally located?\n",
      "num tokens: 8\n",
      "input_ids: [2840, 47, 8, 690, 5330, 1069, 58, 1]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens: ['▁Where', '▁was', '▁the', '▁city', '▁originally', '▁located', '?', '</s>']\n",
      "context example 99:\n",
      "prompt: You must answer the question using ONLY the information provided in the context below.\n",
      "If the answer cannot be determined from the context, respond with \"Not answerable from the given context.\"\n",
      "Do not use any external knowledge.\n",
      "\n",
      "Context:\n",
      "Founded in 1670 as Charles Town in honor of King Charles II of England, Charleston adopted its present name in 1783. It moved to its present location on Oyster Point in 1680 from a location on the west bank of the Ashley River known as Albemarle Point. By 1690, Charles Town was the fifth-largest city in North America, and it remained among the 10 largest cities in the United States through the 1840 census. With a 2010 census population of 120,083  (and a 2014 estimate of 130,113), current trends put Charleston as the fastest-growing municipality in South Carolina. The population of the Charleston metropolitan area, comprising Berkeley, Charleston, and Dorchester Counties, was counted by the 2014 estimate at 727,689 – the third-largest in the state – and the 78th-largest metropolitan statistical area in the United States.\n",
      "\n",
      "Question:\n",
      "Where was the city originally located?\n",
      "num tokens: 254\n",
      "input_ids: [148, 398, 1525, 8, 822, 338, 20728, 8, 251, 937, 16, 8, 2625, 666, 5, 156, 8, 1525, 1178, 36, 4187, 45, 8, 2625, 6, 3531, 28, 96, 10358, 1525]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens: ['▁You', '▁must', '▁answer', '▁the', '▁question', '▁using', '▁ONLY', '▁the', '▁information', '▁provided', '▁in', '▁the', '▁context', '▁below', '.', '▁If', '▁the', '▁answer', '▁cannot', '▁be', '▁determined', '▁from', '▁the', '▁context', ',', '▁respond', '▁with', '▁\"', 'Not', '▁answer']\n"
     ]
    }
   ],
   "source": [
    "def inspect_tokenized(tokenized, prompts, label, idx=99):\n",
    "    if not tokenized:\n",
    "        print(f\"{label}: no items\")\n",
    "        return\n",
    "    idx = max(0, min(idx, len(tokenized) - 1))\n",
    "    item = tokenized[idx]\n",
    "    ids = item[\"input_ids\"]\n",
    "    mask = item[\"attention_mask\"]\n",
    "    print(f\"{label} example {idx}:\")\n",
    "    print(\"prompt:\", prompts[idx])\n",
    "    print(\"num tokens:\", len(ids))\n",
    "    print(\"input_ids:\", ids[:30])\n",
    "    print(\"attention_mask:\", mask[:30])\n",
    "    print(\"tokens:\", tokenizer.convert_ids_to_tokens(ids[:30]))\n",
    "\n",
    "inspect_tokenized(base_tokenized, base_prompts, \"base\", idx=99)\n",
    "inspect_tokenized(context_tokenized, context_prompts, \"context\", idx=99)  # Fixed: was context_questions, now context_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f13581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-eafe0178-ba02-4cc5-8dfe-10d2e59b0f8b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXPERT</th>\n",
       "      <th>BASE_MEAN</th>\n",
       "      <th>CONTEXT_MEAN</th>\n",
       "      <th>DELTA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.163211</td>\n",
       "      <td>0.161690</td>\n",
       "      <td>-0.001520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.064267</td>\n",
       "      <td>0.006406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.181530</td>\n",
       "      <td>0.178404</td>\n",
       "      <td>-0.003126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.133045</td>\n",
       "      <td>0.123711</td>\n",
       "      <td>-0.009335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.077007</td>\n",
       "      <td>0.080151</td>\n",
       "      <td>0.003144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.110977</td>\n",
       "      <td>0.099015</td>\n",
       "      <td>-0.011962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.124609</td>\n",
       "      <td>0.131001</td>\n",
       "      <td>0.006393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.151760</td>\n",
       "      <td>0.161761</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eafe0178-ba02-4cc5-8dfe-10d2e59b0f8b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-eafe0178-ba02-4cc5-8dfe-10d2e59b0f8b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-eafe0178-ba02-4cc5-8dfe-10d2e59b0f8b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   EXPERT  BASE_MEAN  CONTEXT_MEAN     DELTA\n",
       "0       0   0.163211      0.161690 -0.001520\n",
       "1       1   0.057861      0.064267  0.006406\n",
       "2       2   0.181530      0.178404 -0.003126\n",
       "3       3   0.133045      0.123711 -0.009335\n",
       "4       4   0.077007      0.080151  0.003144\n",
       "5       5   0.110977      0.099015 -0.011962\n",
       "6       6   0.124609      0.131001  0.006393\n",
       "7       7   0.151760      0.161761  0.010000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def get_router_logits_for_prompt(prompt):\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = encoding[\"input_ids\"].to(model.device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    classifier_outputs = []\n",
    "\n",
    "    def hook(module, module_inputs, module_outputs):\n",
    "        if isinstance(module_outputs, tuple):\n",
    "            module_outputs = module_outputs[0]\n",
    "        classifier_outputs.append(module_outputs)\n",
    "\n",
    "    handle = router_classifier.register_forward_hook(hook)\n",
    "    with torch.no_grad(), torch.autocast(model.device.type, dtype=model.dtype):\n",
    "        _ = model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    handle.remove()\n",
    "\n",
    "    return classifier_outputs[0].float().cpu(), input_ids[0].cpu().tolist()\n",
    "\n",
    "\n",
    "def find_subsequence(haystack, needle):\n",
    "    if not needle or len(needle) > len(haystack):\n",
    "        return None\n",
    "    for start in range(0, len(haystack) - len(needle) + 1):\n",
    "        if haystack[start:start + len(needle)] == needle:\n",
    "            return start\n",
    "    return None\n",
    "\n",
    "\n",
    "def question_logits_from_context(context_prompt, question_text):\n",
    "    logits, full_ids = get_router_logits_for_prompt(context_prompt)\n",
    "    question_ids = tokenizer(question_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].tolist()\n",
    "    start = find_subsequence(full_ids, question_ids)\n",
    "    if start is None:\n",
    "        return None\n",
    "    end = start + len(question_ids)\n",
    "    return logits[start:end], question_ids\n",
    "\n",
    "\n",
    "def mean_routing_distribution_for_questions(base_questions, context_prompts, context_questions):\n",
    "    total_base_probs = None\n",
    "    total_context_probs = None\n",
    "    total_base_tokens = 0\n",
    "    total_context_tokens = 0\n",
    "\n",
    "    for base_q, context_prompt, context_q in zip(base_questions, context_prompts, context_questions):\n",
    "        base_logits, _ = get_router_logits_for_prompt(base_q)\n",
    "        base_probs = F.softmax(base_logits, dim=-1)\n",
    "        total_base_probs = base_probs.sum(dim=0) if total_base_probs is None else total_base_probs + base_probs.sum(dim=0)\n",
    "        total_base_tokens += base_probs.shape[0]\n",
    "\n",
    "        context_result = question_logits_from_context(context_prompt, context_q)\n",
    "        if context_result is None:\n",
    "            continue\n",
    "        context_logits, _ = context_result\n",
    "        context_probs = F.softmax(context_logits, dim=-1)\n",
    "        total_context_probs = context_probs.sum(dim=0) if total_context_probs is None else total_context_probs + context_probs.sum(dim=0)\n",
    "        total_context_tokens += context_probs.shape[0]\n",
    "\n",
    "    base_mean = total_base_probs / total_base_tokens\n",
    "    context_mean = total_context_probs / total_context_tokens\n",
    "    return base_mean, context_mean\n",
    "\n",
    "\n",
    "base_mean, context_mean = mean_routing_distribution_for_questions(\n",
    "    base_prompts, context_prompts, context_questions\n",
    " )\n",
    "\n",
    "df_mean = pd.DataFrame({\n",
    "    \"EXPERT\": list(range(base_mean.numel())),\n",
    "    \"BASE_MEAN\": base_mean.numpy(),\n",
    "    \"CONTEXT_MEAN\": context_mean.numpy(),\n",
    "})\n",
    "df_mean[\"DELTA\"] = df_mean[\"CONTEXT_MEAN\"] - df_mean[\"BASE_MEAN\"]\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a62c35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁How', '▁many', '▁years', '▁do', '▁mom', 'o', 'tre', 'mes', '▁and', '▁the', 'rian', '▁mammals', '▁go', '▁back', '?']\n",
      "\n",
      "Expert logits for BASE prompt (first 3 question tokens):\n",
      "tensor([[-0.8008, -2.2500,  0.4746, -2.3438, -4.5000, -3.9844,  1.3828,  2.4531],\n",
      "        [ 0.0684, -2.1094, -0.0962, -3.0781, -2.3594, -0.9336,  2.7344,  0.5352],\n",
      "        [ 0.6797,  0.7070,  0.1914, -2.4375,  0.6797, -0.0466, -0.4258,  0.0864],\n",
      "        [ 0.4258, -1.9375,  1.4141, -0.9141,  2.8750, -0.2334,  2.0312,  2.0156],\n",
      "        [-1.5000, -0.7109,  1.1250, -0.9570, -0.3145,  2.7500,  0.3184,  1.2891],\n",
      "        [ 4.8438, -0.8945,  1.9141, -1.7266, -1.5312,  1.0703, -0.4258, -1.1484],\n",
      "        [ 4.1875, -1.8594,  2.0000, -0.9375, -1.7422,  2.5469,  0.4062, -0.4258],\n",
      "        [ 3.9062, -3.7656,  0.3906, -0.9219, -2.6406, -0.9414, -1.6172, -1.9609],\n",
      "        [ 0.0952, -1.0547,  3.4375, -1.8828, -2.6094,  0.3730,  0.8516,  1.2109],\n",
      "        [ 1.0547, -0.1719,  1.1797, -1.9922, -2.5625,  2.4531,  1.0469,  0.4922],\n",
      "        [ 1.8672,  0.3047,  2.4375, -1.4375, -2.4531, -0.2158, -1.0703, -0.5078],\n",
      "        [ 0.9062, -1.7656,  2.3125, -2.3125,  1.7422,  1.0234,  1.4766,  1.5391],\n",
      "        [ 0.8320, -1.3359,  1.5000, -0.5508,  1.5234, -0.7578,  2.2812,  2.0312],\n",
      "        [-0.3848, -1.1406, -0.1030,  1.7578, -1.7031, -0.7461,  0.2734, -1.3672],\n",
      "        [ 1.4297, -0.0698,  4.4062,  0.6719, -1.6875, -2.1406,  0.8984,  1.6406]])\n",
      "\n",
      "Expert logits for CONTEXT prompt (same 3 question tokens):\n",
      "tensor([[-1.7891, -0.4297,  0.4492, -3.1406, -4.8438, -4.2500,  0.7734,  1.9219],\n",
      "        [-0.0757, -1.6641,  0.0869, -4.0625, -2.3594, -1.6719,  2.7344,  0.6680],\n",
      "        [ 0.0737,  1.0156,  0.0918, -2.2344,  0.2754, -0.4473, -0.4414, -0.2930],\n",
      "        [-0.2275, -1.7969,  1.3438, -1.0625,  3.2500, -0.3887,  1.7734,  1.7734],\n",
      "        [-2.5312, -0.2832,  0.7891, -1.3750, -0.5977,  3.1875,  0.2451,  1.4766],\n",
      "        [ 5.2188, -0.6211,  2.0938, -1.2891, -1.9375,  0.6094, -0.1162, -1.4062],\n",
      "        [ 4.5625, -1.2578,  1.6016, -0.6953, -1.7812,  2.2188,  0.6484, -0.8750],\n",
      "        [ 4.5625, -3.9531, -0.0835, -0.9727, -2.4219, -1.2500, -1.5938, -2.2188],\n",
      "        [-0.3867, -1.0938,  3.8906, -1.8125, -2.3438, -0.3125,  0.8047,  1.3672],\n",
      "        [ 0.0830,  0.5586,  0.7969, -2.5938, -2.8594,  2.9375,  1.0156,  0.4688],\n",
      "        [ 2.1094,  0.2119,  1.9453, -1.0859, -3.0625, -0.4688, -1.3672, -1.2656],\n",
      "        [ 0.6562, -1.9766,  1.8828, -2.1562,  1.8516,  0.4766,  1.5000,  1.5469],\n",
      "        [ 0.4434, -1.7266,  1.3750, -0.5000,  1.7500, -1.3359,  2.3125,  2.0625],\n",
      "        [-0.2275, -1.3828, -0.5312,  2.1875, -1.5938, -1.0469,  0.5312, -1.3594],\n",
      "        [ 1.4922,  0.8125,  4.0625,  0.5703, -2.1719, -3.3438,  0.4570,  1.3672]])\n",
      "\n",
      "Top expert changes across question tokens: 1/15\n",
      "Top expert change rate: 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: expert logits for identical question tokens under base vs context\n",
    "sample_idx = min(0, len(base_prompts) - 1, len(context_questions) - 1)\n",
    "if base_prompts and context_prompts and context_questions:\n",
    "    base_text = base_prompts[sample_idx]\n",
    "    context_full = context_prompts[sample_idx]\n",
    "    context_question = context_questions[sample_idx]\n",
    "\n",
    "    context_result = question_logits_from_context(context_full, context_question)\n",
    "    if context_result is None:\n",
    "        print(\"No question slice match found; cannot compare experts.\")\n",
    "    else:\n",
    "        base_logits, base_ids = get_router_logits_for_prompt(base_text)\n",
    "        context_logits, question_ids = context_result\n",
    "\n",
    "        token_text = tokenizer.convert_ids_to_tokens(question_ids)\n",
    "        print(\"Tokens:\", token_text)\n",
    "        print(\"\\nExpert logits for BASE prompt (first 3 question tokens):\")\n",
    "        print(base_logits)\n",
    "        print(\"\\nExpert logits for CONTEXT prompt (same 3 question tokens):\")\n",
    "        print(context_logits)\n",
    "\n",
    "        base_top = base_logits.argmax(dim=-1)\n",
    "        context_top = context_logits.argmax(dim=-1)\n",
    "        top_changes = (base_top != context_top).sum().item()\n",
    "        total_tokens = base_top.numel()\n",
    "        print(\"\\nTop expert changes across question tokens:\", f\"{top_changes}/{total_tokens}\")\n",
    "        print(\"Top expert change rate:\", top_changes / max(1, total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415ddd5",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "The single-example check shows that **adding context shifts routing logits for the same question tokens**. While the token IDs remain identical, the expert preferences change because the model encodes context-dependent representations.\n",
    "\n",
    "To determine if this is a **systematic effect** rather than noise, we now measure how often the top expert assignment changes across **all 100 question tokens** when context is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2347e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total question tokens: 1397\n",
      "Top expert changes: 104\n",
      "Top expert change rate: 0.0744\n"
     ]
    }
   ],
   "source": [
    "# Measure top expert changes across all 100 questions\n",
    "total_changes = 0\n",
    "total_tokens = 0\n",
    "mismatches = 0\n",
    "\n",
    "for base_q, context_prompt, context_q in zip(base_prompts, context_prompts, context_questions):\n",
    "    base_logits, _ = get_router_logits_for_prompt(base_q)\n",
    "    context_result = question_logits_from_context(context_prompt, context_q)\n",
    "    if context_result is None:\n",
    "        mismatches += 1\n",
    "        continue\n",
    "    context_logits, _ = context_result\n",
    "    base_top = base_logits.argmax(dim=-1)\n",
    "    context_top = context_logits.argmax(dim=-1)\n",
    "    total_changes += (base_top != context_top).sum().item()\n",
    "    total_tokens += base_top.numel()\n",
    "\n",
    "print(f\"Total question tokens: {total_tokens}\")\n",
    "print(f\"Top expert changes: {total_changes}\")\n",
    "print(f\"Top expert change rate: {total_changes / max(1, total_tokens):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68721cdb",
   "metadata": {},
   "source": [
    "## Expert Activation Rates and Risk Difference\n",
    "\n",
    "Compute the activation rate for each expert, defined as the proportion of tokens for which that expert is the top-1 (argmax) choice.\n",
    "\n",
    "We compare:\n",
    "- **question_base** (x^(1)): question-only prompts\n",
    "- **question_context** (x^(2)): question tokens within full context prompts\n",
    "\n",
    "The **Risk Difference** Δi = p_i^(1) - p_i^(2) quantifies how much more/less frequently expert i is activated in question_base vs question_context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42a5cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_base: N^(1) = 1397 tokens\n",
      "question_context: N^(2) = 1397 tokens\n",
      "\n",
      "Activation rates sum to 1.0: question_base=1.0000, question_context=1.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e98e6aa4-91fb-4376-ad08-972f34f70876\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Expert</th>\n",
       "      <th>question_base (p^(1))</th>\n",
       "      <th>question_context (p^(2))</th>\n",
       "      <th>Risk Difference (Δi)</th>\n",
       "      <th>question_base Count</th>\n",
       "      <th>question_context Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Expert 0</td>\n",
       "      <td>0.142448</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Expert 1</td>\n",
       "      <td>0.050107</td>\n",
       "      <td>0.052255</td>\n",
       "      <td>-0.002147</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Expert 2</td>\n",
       "      <td>0.136722</td>\n",
       "      <td>0.132427</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>191</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Expert 3</td>\n",
       "      <td>0.132427</td>\n",
       "      <td>0.127416</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>185</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expert 4</td>\n",
       "      <td>0.133858</td>\n",
       "      <td>0.143164</td>\n",
       "      <td>-0.009306</td>\n",
       "      <td>187</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Expert 5</td>\n",
       "      <td>0.129563</td>\n",
       "      <td>0.120974</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>181</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Expert 6</td>\n",
       "      <td>0.110952</td>\n",
       "      <td>0.114531</td>\n",
       "      <td>-0.003579</td>\n",
       "      <td>155</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Expert 7</td>\n",
       "      <td>0.163923</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>-0.003579</td>\n",
       "      <td>229</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e98e6aa4-91fb-4376-ad08-972f34f70876')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e98e6aa4-91fb-4376-ad08-972f34f70876 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e98e6aa4-91fb-4376-ad08-972f34f70876');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     Expert  question_base (p^(1))  question_context (p^(2))  \\\n",
       "0  Expert 0               0.142448                  0.141732   \n",
       "1  Expert 1               0.050107                  0.052255   \n",
       "2  Expert 2               0.136722                  0.132427   \n",
       "3  Expert 3               0.132427                  0.127416   \n",
       "4  Expert 4               0.133858                  0.143164   \n",
       "5  Expert 5               0.129563                  0.120974   \n",
       "6  Expert 6               0.110952                  0.114531   \n",
       "7  Expert 7               0.163923                  0.167502   \n",
       "\n",
       "   Risk Difference (Δi)  question_base Count  question_context Count  \n",
       "0              0.000716                  199                     198  \n",
       "1             -0.002147                   70                      73  \n",
       "2              0.004295                  191                     185  \n",
       "3              0.005011                  185                     178  \n",
       "4             -0.009306                  187                     200  \n",
       "5              0.008590                  181                     169  \n",
       "6             -0.003579                  155                     160  \n",
       "7             -0.003579                  229                     234  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute expert activation rates for question_base and question_context\n",
    "\n",
    "num_experts = 8\n",
    "\n",
    "# 1. question_base (x^(1)): question-only prompts\n",
    "question_base_activations = torch.zeros(num_experts, dtype=torch.long)\n",
    "question_base_total_tokens = 0\n",
    "\n",
    "for base_prompt in base_prompts:\n",
    "    logits, _ = get_router_logits_for_prompt(base_prompt)\n",
    "    top_experts = torch.argmax(logits, dim=1)  # [num_tokens]\n",
    "    \n",
    "    for expert_idx in range(num_experts):\n",
    "        question_base_activations[expert_idx] += (top_experts == expert_idx).sum().item()\n",
    "    \n",
    "    question_base_total_tokens += len(top_experts)\n",
    "\n",
    "# 2. question_context (x^(2)): question tokens within full context prompts\n",
    "question_context_activations = torch.zeros(num_experts, dtype=torch.long)\n",
    "question_context_total_tokens = 0\n",
    "\n",
    "for context_prompt, context_question in zip(context_prompts, context_questions):\n",
    "    # Get question logits within the full context\n",
    "    result = question_logits_from_context(context_prompt, context_question)\n",
    "    if result is None:\n",
    "        continue\n",
    "    \n",
    "    question_logits, _ = result\n",
    "    \n",
    "    # Top experts for question tokens only\n",
    "    top_experts = torch.argmax(question_logits, dim=1)  # [num_question_tokens]\n",
    "    \n",
    "    for expert_idx in range(num_experts):\n",
    "        question_context_activations[expert_idx] += (top_experts == expert_idx).sum().item()\n",
    "    \n",
    "    question_context_total_tokens += len(top_experts)\n",
    "\n",
    "# Compute activation rates\n",
    "question_base_rates = question_base_activations.float() / question_base_total_tokens\n",
    "question_context_rates = question_context_activations.float() / question_context_total_tokens\n",
    "\n",
    "# Compute Risk Difference\n",
    "rd = question_base_rates - question_context_rates\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_activation_rates = pd.DataFrame({\n",
    "    'Expert': [f'Expert {i}' for i in range(num_experts)],\n",
    "    'question_base (p^(1))': question_base_rates.numpy(),\n",
    "    'question_context (p^(2))': question_context_rates.numpy(),\n",
    "    'Risk Difference (Δi)': rd.numpy(),\n",
    "    'question_base Count': question_base_activations.numpy(),\n",
    "    'question_context Count': question_context_activations.numpy(),\n",
    "})\n",
    "\n",
    "print(f\"question_base: N^(1) = {question_base_total_tokens} tokens\")\n",
    "print(f\"question_context: N^(2) = {question_context_total_tokens} tokens\")\n",
    "print(f\"\\nActivation rates sum to 1.0: question_base={question_base_rates.sum():.4f}, question_context={question_context_rates.sum():.4f}\\n\")\n",
    "\n",
    "df_activation_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807e829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
