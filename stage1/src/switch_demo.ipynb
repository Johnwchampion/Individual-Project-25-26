{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d07dd09",
   "metadata": {},
   "source": [
    "# Preliminary MoE Routing Demo (Switch Transformer)\n",
    "\n",
    "This notebook is a small exploratory test to understand **Mixture-of-Experts (MoE) routing** in a real model.\n",
    "\n",
    "Using **Switch-Base-8**, we inspect **token-level expert routing decisions** at a **single encoder MoE layer**.  \n",
    "We capture router logits, selected experts, and routing entropy for short inputs, and align them with the tokenizer’s actual subword tokens.\n",
    "\n",
    "This notebook is **purely observational**:\n",
    "- no training\n",
    "- no fine-tuning\n",
    "- no architectural modification\n",
    "\n",
    "The goal is simply to verify that expert routing can be intercepted and interpreted before scaling to larger MoE models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbe00b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b292d5746e48bdb2297783bf0aa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b292d5746e48bdb2297783bf0aa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b292d5746e48bdb2297783bf0aa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch MoE loaded correctly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b292d5746e48bdb2297783bf0aa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch MoE loaded correctly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-auto_conversion:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 116, in auto_conversion\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 95, in auto_conversion\n",
      "    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 71, in get_conversion_pr_reference\n",
      "    spawn_conversion(token, private, model_id)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 48, in spawn_conversion\n",
      "    event_id = result[\"event_id\"]\n",
      "               ~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'event_id'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/switch-base-8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Switch MoE loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd20e5d",
   "metadata": {},
   "source": [
    "List all modules with \"router\" in the name to find the routing components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d36df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.1.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.1.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.3.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.3.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.5.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.5.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.7.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.7.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.9.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.9.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.block.11.layer.1.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "encoder.block.11.layer.1.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.1.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.1.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.3.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.3.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.5.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.5.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.7.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.7.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.9.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.9.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "decoder.block.11.layer.2.mlp.router -> <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router'>\n",
      "decoder.block.11.layer.2.mlp.router.classifier -> <class 'torch.nn.modules.linear.Linear'>\n",
      "Switch MoE router modules identified.\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"router\" in name.lower():\n",
    "        print(name, \"->\", type(module))\n",
    "print(\"Switch MoE router modules identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58b237",
   "metadata": {},
   "source": [
    "Tokenize a single example and print ids, tokens, and attention mask to see the subword split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3d9cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "question: Where is Paris?, context: Paris is the capital of France.\n",
      "\n",
      "Token IDs:\n",
      "[822, 10, 2840, 19, 1919, 58, 6, 2625, 10, 1919, 19, 8, 1784, 13, 1410, 5, 1]\n",
      "\n",
      "Tokens:\n",
      "['▁question', ':', '▁Where', '▁is', '▁Paris', '?', ',', '▁context', ':', '▁Paris', '▁is', '▁the', '▁capital', '▁of', '▁France', '.', '</s>']\n",
      "\n",
      "Attention Mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Minimal step: tokenize a single input and inspect tokens\n",
    "text = \"question: Where is Paris?, context: Paris is the capital of France.\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"][0].tolist()\n",
    "attention_mask = encoding[\"attention_mask\"][0].tolist()\n",
    "\n",
    "print(\"Input text:\")\n",
    "print(text)\n",
    "print(\"\\nToken IDs:\")\n",
    "print(input_ids)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd43a6",
   "metadata": {},
   "source": [
    "Move tokenized tensors onto the same device as the model. The model might be on GPU while the tokenizer outputs are on CPU, and PyTorch requires both to be on the same device for a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef483bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "attention_mask device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move tokenized tensors to the model device\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"].to(model.device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(model.device)\n",
    "\n",
    "print(\"input_ids device:\", input_ids.device)\n",
    "print(\"attention_mask device:\", attention_mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b914dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output last_hidden_state: torch.Size([1, 17, 768])\n",
      "This consists of the batch dimension (1), sequence length, and hidden state dimension.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Encoder forward only (no hooks)\n",
    "with torch.no_grad(), torch.autocast(model.device.type, dtype=model.dtype):\n",
    "    encoder_outputs = model.encoder(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "print(\"Encoder output last_hidden_state:\", encoder_outputs.last_hidden_state.shape)\n",
    "print(\"This consists of the batch dimension (1), sequence length, and hidden state dimension.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e9194",
   "metadata": {},
   "source": [
    "\n",
    "### Forward hook method\n",
    "\n",
    "\n",
    "- **`router_classifier`**  \n",
    "  The **linear layer inside the router** that maps each token’s hidden state to logits over the experts  \n",
    "  (shape: `(hidden_dim, num_experts)`).\n",
    "\n",
    "- **`classifier_outputs`**  \n",
    "  A list used to **store the outputs captured by the hook**.  \n",
    "  Hooks may fire multiple times, so outputs are accumulated safely here.\n",
    "\n",
    "- **`classifier_hook(module, module_inputs, module_outputs)`**  \n",
    "  A function that PyTorch **automatically calls when the classifier runs**.\n",
    "  - `module`: the classifier layer itself (`nn.Linear`)\n",
    "  - `module_inputs`: hidden-state tensors **entering the classifier**  \n",
    "    (one vector per token)\n",
    "  - `module_outputs`: **raw logits over experts**  \n",
    "    (shape: `[num_tokens, num_experts] so (17,8) `)\n",
    "\n",
    "- **Tuple handling inside the hook**  \n",
    "  Some modules return `(output, extra_info)`.  \n",
    "  This unwraps the tensor so we always store just the logits.\n",
    "\n",
    "- **`register_forward_hook(...)`**  \n",
    "  Attaches the hook to the classifier.  \n",
    "  From this point on, **whenever the classifier executes**, the hook is triggered.\n",
    "\n",
    "- **`with torch.no_grad()`**  \n",
    "  Disables gradient tracking because we are **observing, not training**.  \n",
    "  Saves memory and computation.\n",
    "\n",
    "- **`torch.autocast(...)`**  \n",
    "  Runs the forward pass in the model’s native precision (e.g. FP16 on GPU).  \n",
    "  Prevents dtype mismatches and is standard practice for inference.\n",
    "\n",
    "- **`model.encoder(...)`**  \n",
    "  Executes a single encoder forward pass.  \n",
    "  This is what **actually triggers the router and the hook**.\n",
    "\n",
    "- **`handle.remove()`**  \n",
    "  Detaches the hook immediately after the pass to avoid duplicate captures.\n",
    "\n",
    "- **`classifier_outputs[0]`**  \n",
    "  The captured tensor of **per-token expert logits**.  \n",
    "  Each row corresponds to one token, each column to one expert.\n",
    "\n",
    "- **Final prints**  \n",
    "  Confirm tensor shape and inspect logits for a specific token and all experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aecb2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier logits shape: (17, 8)\n",
      "Classifier logits sample: [0.05078125, -2.0, -0.17578125, 2.8125, -0.154296875, -0.173828125, 0.470703125, 0.279296875]\n"
     ]
    }
   ],
   "source": [
    "# Hook the router classifier to capture per-expert logits\n",
    "router_classifier = model.encoder.block[1].layer[1].mlp.router.classifier\n",
    "classifier_outputs = []\n",
    "\n",
    "# Define a hook function to capture the outputs of the router classifier\n",
    "def classifier_hook(module, module_inputs, module_outputs):\n",
    "    if isinstance(module_outputs, tuple):\n",
    "        #Ensure we only capture the logits \n",
    "        module_outputs = module_outputs[0]\n",
    "    classifier_outputs.append(module_outputs)\n",
    "\n",
    "\n",
    "handle = router_classifier.register_forward_hook(classifier_hook)\n",
    "\n",
    "with torch.no_grad(), torch.autocast(model.device.type, dtype=model.dtype):\n",
    "    _ = model.encoder(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "logits = classifier_outputs[0].cpu()\n",
    "print(\"Classifier logits shape:\", tuple(logits.shape))\n",
    "print(\"Classifier logits sample:\", logits[13, :8].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed03e3f",
   "metadata": {},
   "source": [
    "Convert logits into a concrete routing summary. We take the argmax to pick the top expert per token, compute entropy to measure confidence, then align those values with the tokenizer's subword tokens in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf5c222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2386a94e-9897-4cb0-a222-b60208a60a8b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>EXPERT</th>\n",
       "      <th>LOGIT</th>\n",
       "      <th>ENTROPY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁question</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.733095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:</td>\n",
       "      <td>2</td>\n",
       "      <td>2.468750</td>\n",
       "      <td>1.472535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁Where</td>\n",
       "      <td>7</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>1.368643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁is</td>\n",
       "      <td>4</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>1.297853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁Paris</td>\n",
       "      <td>7</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>1.361608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2.359375</td>\n",
       "      <td>1.484937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>1.598489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>▁context</td>\n",
       "      <td>5</td>\n",
       "      <td>2.656250</td>\n",
       "      <td>1.720571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>:</td>\n",
       "      <td>2</td>\n",
       "      <td>2.796875</td>\n",
       "      <td>1.355670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>▁Paris</td>\n",
       "      <td>7</td>\n",
       "      <td>2.921875</td>\n",
       "      <td>1.281017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>▁is</td>\n",
       "      <td>4</td>\n",
       "      <td>2.453125</td>\n",
       "      <td>1.445303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>▁the</td>\n",
       "      <td>6</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>1.566154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>▁capital</td>\n",
       "      <td>4</td>\n",
       "      <td>2.468750</td>\n",
       "      <td>1.167180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>▁of</td>\n",
       "      <td>3</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.117690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>▁France</td>\n",
       "      <td>7</td>\n",
       "      <td>2.468750</td>\n",
       "      <td>1.105413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>2</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>1.586292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>1.226562</td>\n",
       "      <td>1.775595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2386a94e-9897-4cb0-a222-b60208a60a8b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2386a94e-9897-4cb0-a222-b60208a60a8b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2386a94e-9897-4cb0-a222-b60208a60a8b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        TOKEN  EXPERT     LOGIT   ENTROPY\n",
       "0   ▁question       4  2.000000  1.733095\n",
       "1           :       2  2.468750  1.472535\n",
       "2      ▁Where       7  2.375000  1.368643\n",
       "3         ▁is       4  2.718750  1.297853\n",
       "4      ▁Paris       7  2.265625  1.361608\n",
       "5           ?       2  2.359375  1.484937\n",
       "6           ,       2  2.062500  1.598489\n",
       "7    ▁context       5  2.656250  1.720571\n",
       "8           :       2  2.796875  1.355670\n",
       "9      ▁Paris       7  2.921875  1.281017\n",
       "10        ▁is       4  2.453125  1.445303\n",
       "11       ▁the       6  1.812500  1.566154\n",
       "12   ▁capital       4  2.468750  1.167180\n",
       "13        ▁of       3  2.812500  1.117690\n",
       "14    ▁France       7  2.468750  1.105413\n",
       "15          .       2  1.953125  1.586292\n",
       "16       </s>       7  1.226562  1.775595"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute expert assignment + entropy per token\n",
    "logits = logits.float()\n",
    "experts = logits.argmax(dim=-1)\n",
    "entropy = -(F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)).sum(dim=-1)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "df = pd.DataFrame({\n",
    "    \"TOKEN\": tokens,\n",
    "    \"EXPERT\": experts.cpu().numpy(),\n",
    "    \"LOGIT\": logits.max(dim=-1).values.cpu().numpy(),\n",
    "    \"ENTROPY\": entropy.cpu().numpy()\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48defb64",
   "metadata": {},
   "source": [
    "Next we prepare the contents of Prompts_base and Prompts_context for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8265ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(\"/content/drive/MyDrive/Individual-Project-25-26/stage1/data\")\n",
    "base_path = data_dir / \"Prompt_base.jsonl\"\n",
    "context_path = data_dir / \"Prompt_context.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f392372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base prompts: 100\n",
      "Loaded context prompts: 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_prompts(path):\n",
    "    prompts = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            prompts.append(obj[\"prompt\"])\n",
    "    return prompts\n",
    "\n",
    "base_prompts = load_prompts(base_path)\n",
    "context_prompts = load_prompts(context_path)\n",
    "\n",
    "prompt_sets = {\n",
    "    \"base\": base_prompts,\n",
    "    \"context\": context_prompts,\n",
    "}\n",
    "\n",
    "print(\"Loaded base prompts:\", len(base_prompts))\n",
    "print(\"Loaded context prompts:\", len(context_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c731df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base tokenized: 100\n",
      "Context tokenized: 100\n"
     ]
    }
   ],
   "source": [
    "def tokenize_prompts(prompts):\n",
    "    tokenized = []\n",
    "    for prompt in prompts:\n",
    "        encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        tokenized.append({\n",
    "            \"input_ids\": encoding[\"input_ids\"][0].tolist(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"][0].tolist(),\n",
    "        })\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "\n",
    "base_tokenized = tokenize_prompts(base_prompts)\n",
    "context_tokenized = tokenize_prompts(context_prompts)\n",
    "\n",
    "print(\"Base tokenized:\", len(base_tokenized))\n",
    "print(\"Context tokenized:\", len(context_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241075f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base example 99:\n",
      "prompt: Where was the city originally located?\n",
      "num tokens: 8\n",
      "input_ids: [2840, 47, 8, 690, 5330, 1069, 58, 1]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens: ['▁Where', '▁was', '▁the', '▁city', '▁originally', '▁located', '?', '</s>']\n",
      "context example 0:\n",
      "prompt: You must answer the question using ONLY the information provided in the context below.\n",
      "If the answer cannot be determined from the context, respond with \"Not answerable from the given context.\"\n",
      "Do not use any external knowledge.\n",
      "\n",
      "Context:\n",
      "If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. Ambondro is more closely related to monotremes than to therian mammals while Amphilestes and Amphitherium are more closely related to the therians; as fossils of all three genera are dated about 167 million years ago in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group. The earliest known synapsid satisfying Kemp's definitions is Tikitherium, dated 225 Ma, so the appearance of mammals in this broader sense can be given this Late Triassic date. In any case, the temporal range of the group extends to the present day.\n",
      "\n",
      "Question:\n",
      "How many years do momotremes and therian mammals go back?\n",
      "num tokens: 240\n",
      "input_ids: [148, 398, 1525, 8, 822, 338, 20728, 8, 251, 937, 16, 8, 2625, 666, 5, 156, 8, 1525, 1178, 36, 4187, 45, 8, 2625, 6, 3531, 28, 96, 10358, 1525]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens: ['▁You', '▁must', '▁answer', '▁the', '▁question', '▁using', '▁ONLY', '▁the', '▁information', '▁provided', '▁in', '▁the', '▁context', '▁below', '.', '▁If', '▁the', '▁answer', '▁cannot', '▁be', '▁determined', '▁from', '▁the', '▁context', ',', '▁respond', '▁with', '▁\"', 'Not', '▁answer']\n"
     ]
    }
   ],
   "source": [
    "def inspect_tokenized(tokenized, prompts, label, idx=99):\n",
    "    if not tokenized:\n",
    "        print(f\"{label}: no items\")\n",
    "        return\n",
    "    idx = max(0, min(idx, len(tokenized) - 1))\n",
    "    item = tokenized[idx]\n",
    "    ids = item[\"input_ids\"]\n",
    "    mask = item[\"attention_mask\"]\n",
    "    print(f\"{label} example {idx}:\")\n",
    "    print(\"prompt:\", prompts[idx])\n",
    "    print(\"num tokens:\", len(ids))\n",
    "    print(\"input_ids:\", ids[:30])\n",
    "    print(\"attention_mask:\", mask[:30])\n",
    "    print(\"tokens:\", tokenizer.convert_ids_to_tokens(ids[:30]))\n",
    "\n",
    "inspect_tokenized(base_tokenized, base_prompts, \"base\", idx=99)\n",
    "inspect_tokenized(context_tokenized, context_prompts, \"context\", idx=99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
